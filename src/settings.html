<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Neural Privacy Policy Settings</title>
        <link rel="stylesheet" href="styles/settings.css">
    </head>
    <body>
        <div id="settings-container">

            <h1>Neural Privacy Policy</h1>

            <div class="settings-item">
                <p class="settings-description">
                    Neural Privacy Policy is a liBERTa-powered browser extension that automatically classifies and
                    analyzes website privacy policies directly within the browser. Built on a modular architecture,
                    the extension extracts privacy policy content from Web pages in real time, segments the text,
                    and classifies each segment into predefined privacy-related categories using a pre-trained
                    Transformer model optimized for in-browser execution. This fully client-side approach enhances
                    user privacy by ensuring that no data leaves the user's device during the process, while providing
                    real-time, structured insights into key data practices such as first-party data collection,
                    third-party sharing, user rights, and security measures.
                </p>
                <p class="settings-description">
                    This extension is part of the liBERTa framework, a general-purpose client-side infrastructure
                    designed to support real-time Machine Learning and Deep Learning models directly in the browser.
                </p>
                <p class="settings-description">
                    The scientific paper describing the system architecture, methodology, and performance evaluation is
                    available at: <a>DOI</a>.
                </p>
            </div>

            <div class="settings-item">
                <label class="settings-title" for="settings-batch-size">Batch size</label>
                <p class="settings-description">
                    This defines the number of input segments processed simultaneously by the model. A larger batch
                    size can improve performance by reducing the number of iterations required to process all segments.
                    However, it may also increase memory usage and inference time.
                </p>
                <input id="settings-batch-size" type="number" class="user-control" min=1 value=1>
                <p id="settings-batch-size-disclaimer" class="settings-note">
                    <b>Note</b>: The batch size should be set based on the available system resources and the desired
                    trade-off between performance and memory usage. A larger batch size can speed up processing but may
                    require more memory. Conversely, a smaller batch size may be slower but consume less memory.
                </p>
            </div>

            <div class="settings-item">
                <label class="settings-title" for="settings-max-seq-len-slider">Max Sequence Length</label>
                <p class="settings-description">
                    This defines the maximum number of tokens the model can process. If a privacy policy segment exceeds
                    this limit, the text will be truncated to fit within the specified sequence length.
                </p>

                <div class="slider-container">
                    <input id="settings-max-seq-len-slider" type="range" min="1" max="512" value="128" class="slider">
                    <label id="settings-max-seq-len-label" class="slider-value">128</label>
                </div>
                <p id="sequence-length-disclaimer" class="settings-note">
                    <b>Note</b>: A longer maximum sequence length allows the model to capture broader contexts, which
                    may improve comprehension of extended text. However, it can also increase memory usage and inference
                    time. On the other hand, shorter sequence lengths reduce the context window, potentially leading to
                    information loss, but they improve performance by speeding up the inference process.
                </p>
            </div>

            <div class="settings-item">
                <label class="settings-title" for="settings-model-name">Model</label>
                <p class="settings-description">
                    The deep learning model used to perform the classification.
                </p>
                <select id="settings-model-name" class="user-control" name="models">

                </select>
                <p class="settings-note">
                    <b>Note</b>: If the selected model has not been downloaded previously, it will be automatically
                    downloaded when you apply the settings. Ensure you have an active internet connection during this
                    process.
                </p>
            </div>

            <div class="settings-item">
                <label class="settings-title" for="settings-model-size">Model size</label>
                <p class="settings-description">
                    The size refers to the complexity of the model, measured in terms of the number of layers, hidden
                    units, and overall parameters.
                </p>
                <select id="settings-model-size" class="user-control" name="model-sizes">

                </select>
                <p class="settings-note">
                    <b>Note</b>: Larger models offer improved performance but require more computational resources.
                </p>
            </div>

            <div class="settings-item">
                <label class="settings-title" for="settings-hw-acceleration">Hardware acceleration</label>
                <p class="settings-description">
                    Hardware acceleration uses GPUs for faster model inference. If unavailable, the CPU is used.
                </p>
                <select id="settings-hw-acceleration" class="user-control" name="device" aria-label="Select compute device">
                    <option value="true">On</option>
                    <option value="false">Off</option>
                </select>
                <p class="settings-note">
                    <b>Note</b>: If a GPU is not detected, the CPU will be used by default. GPUs can significantly
                    accelerate model inference.
                </p>
            </div>

            <div class="buttons-container">
                <button id="save-button">Save</button>
                <button id="reset-button">Reset</button>
            </div>

            <div id="status-section">
                <div id="progress-bar-container"></div>
                <p id="status-message"></p>
            </div>

            <div id="info-section">
                <div class="info-row">
                    <p id="info-version" class="info-cell">v<%= version %></p>
                    <p id="info-github" class="info-cell">
                        <a target="_blank">GitHub</a>
                    </p>
                    <p id="info-terms-of-service" class="info-cell">
                        <a href="https://www.eclipse.org/legal/epl-2.0/" target="_blank">License</a>
                    </p>
                </div>
            </div>
        </div>

        <script src="scripts/settings_ui.js" type="module"></script>
    </body>
</html>
